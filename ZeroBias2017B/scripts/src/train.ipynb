{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waitGPU: Waiting for the following conditions, checking every 10 seconds. \n",
      "Failed to import waitGPU\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import waitGPU\n",
    "    waitGPU.wait(ngpu=1)\n",
    "except:\n",
    "    print(\"Failed to import waitGPU\")\n",
    "\n",
    "from utils import do_tsne, do_pca, do_gif, df_plot, df_plot2\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackBox:\n",
    "    def __init__(self, data_dir, working_dir, threshold=0.9):\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "        dt = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        current_dir = f\"t-{threshold}-{dt}\"\n",
    "\n",
    "        self.working_dir = Path(working_dir).joinpath(current_dir)\n",
    "        self.working_dir.mkdir(parents=True)\n",
    "\n",
    "        print(f\"Working dir: {self.working_dir}\")\n",
    "\n",
    "        self.df_train = None\n",
    "        self.df_test = None\n",
    "        self.bin_cols = []\n",
    "\n",
    "        self.df_anomalies = None\n",
    "        self.threshold = threshold\n",
    "\n",
    "        self.training_performance = []\n",
    "        self.model = None\n",
    "\n",
    "    def load_df(self, nbins=100, step=1):\n",
    "\n",
    "        self.df_train = pd.read_csv(\n",
    "            Path(self.data_dir, 'train.csv'))   # Hand picked data\n",
    "        self.df_test = pd.read_csv(\n",
    "            Path(self.data_dir, 'test.csv'))  # Data to be classified\n",
    "\n",
    "        print(\n",
    "            f\"Train set {self.df_train.shape} | Test set {self.df_test.shape}\")\n",
    "\n",
    "        # Filter list of columns which will be used for training\n",
    "        self.bin_cols = [col for col in self.df_train.columns if 'bin_' in col]\n",
    "\n",
    "        # Remove first and last values as those are over/under flows\n",
    "        self.bin_cols = self.bin_cols[1:-1]\n",
    "\n",
    "        if nbins > len(self.bin_cols):\n",
    "            nbins = len(self.bin_cols)\n",
    "\n",
    "        self.bin_cols = [f\"bin_{i}\" for i in range(1, nbins, step)]\n",
    "\n",
    "    def create_model(self):\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.Dense(units=50, activation=\"relu\", input_shape=(len(self.bin_cols),)))\n",
    "        model.add(layers.Dropout(0.1))\n",
    "        model.add(layers.Dense(units=25, activation=\"relu\"))\n",
    "        model.add(layers.Dropout(0.1))\n",
    "        model.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "        model.compile(optimizer=\"adam\",\n",
    "                      loss=\"binary_crossentropy\",\n",
    "                      metrics=[\"accuracy\"])\n",
    "        return model\n",
    "\n",
    "    def train_model(self, df, train_index):\n",
    "        \"\"\" Trains new model with a given dataset\n",
    "            Params:\n",
    "                df (pandas.DataFrame) - dataset for training new model\n",
    "\n",
    "            Returns:\n",
    "                model (keras.Sequential) - trained model\n",
    "        \"\"\"\n",
    "        try:\n",
    "            train_start_time = time.time()\n",
    "            # Normalization, divide every bin value by total entries\n",
    "            X = df.filter(self.bin_cols, axis=1).copy().div(df.entries, axis=0)\n",
    "            y = df[\"y\"]\n",
    "\n",
    "            # Stratified shuffle split\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, stratify=y, test_size=0.25)\n",
    "\n",
    "            X_train = np.asarray(X_train)\n",
    "            X_test = np.asarray(X_test)\n",
    "            y_train = np.asarray(y_train)\n",
    "            y_test = np.asarray(y_test)\n",
    "\n",
    "            # Calculate class weights\n",
    "            cw = class_weight.compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "            cw = dict(enumerate(cw))\n",
    "#             print(cw)\n",
    "\n",
    "            # Train\n",
    "            es = EarlyStopping(monitor='loss', mode='min', patience=5, verbose=1)\n",
    "            log_dir = self.working_dir.joinpath(\"tensorboard\").joinpath(str(train_index))\n",
    "            tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch=0)\n",
    "            \n",
    "            model = self.create_model()\n",
    "            model.fit(X_train, y_train, verbose=0,\n",
    "                      batch_size=64,\n",
    "                      validation_split=0.25,\n",
    "                      epochs=1000,\n",
    "                      shuffle=True,\n",
    "                      callbacks=[es, tensorboard_callback],\n",
    "                      class_weight=cw)\n",
    "\n",
    "            # Predict\n",
    "            y_pred_ = model.predict(X_test)\n",
    "            y_pred = (y_pred_ > 0.5)\n",
    "\n",
    "            self.training_performance.append({\n",
    "                \"acc\": accuracy_score(y_test, y_pred),\n",
    "                \"f1\": f1_score(y_test, y_pred),\n",
    "                \"df_size\": df.shape[0],\n",
    "                \"ngood\": df[df[\"y\"] == 1].shape[0],\n",
    "                \"nbad\": df[df[\"y\"] == 0].shape[0],\n",
    "                \"confusion_matrix\": confusion_matrix(y_test, y_pred).tolist(),\n",
    "                \"train_time\": int(time.time()-train_start_time)\n",
    "#                 , \"class_weight\": cw\n",
    "            })\n",
    "        except Exception as err:\n",
    "            print(\"Failed train_model |\", err)\n",
    "            raise err\n",
    "\n",
    "        return model\n",
    "\n",
    "    def predict_run(self, df_run):\n",
    "        \"\"\" Does prediction with given model and dataset\n",
    "\n",
    "            df_run (pd.DataFrame) - dataset\n",
    "\n",
    "            Returns:\n",
    "                df_run (pd.DataFrame) - dataset with additional two columns: \n",
    "                                        y_pred - model prediciton [0..1]\n",
    "                                        y with - label (1 good, 0 bad, 2 anomaly)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            # Scale each row by dividing by number of total entries within a run\n",
    "            X = df_run.filter(self.bin_cols, axis=1).div(\n",
    "                df_run.entries, axis=0)\n",
    "            X = np.asarray(X)\n",
    "\n",
    "            y_pred = self.model.predict(X)\n",
    "\n",
    "            # Predicted label by ANN\n",
    "            df_run[\"y_pred\"] = y_pred\n",
    "\n",
    "            # Create new column y and set final label there\n",
    "\n",
    "            # Predictions with higher probability than threshold are considered as GOOD\n",
    "            filter_good = df_run['y_pred'] >= self.threshold\n",
    "            # Predictions with lower probability than threshold are considered as BAD\n",
    "            filter_bad = df_run['y_pred'] <= 1-self.threshold\n",
    "            # Predictions between lower and higher thresholds are considered as ANOMALIES\n",
    "            filter_anon = (\n",
    "                1-self.threshold < df_run['y_pred']) & (df_run['y_pred'] < self.threshold)\n",
    "\n",
    "            # Create new column y and set final label there\n",
    "            df_run.loc[filter_good, 'y'] = 1\n",
    "            df_run.loc[filter_bad, 'y'] = 0\n",
    "            df_run.loc[filter_anon, 'y'] = 2\n",
    "            df_run = df_run.astype({\"y\": \"int32\"})\n",
    "\n",
    "        except Exception as err:\n",
    "            print(\"Failed predict_run |\", err)\n",
    "            raise err\n",
    "\n",
    "        return df_run\n",
    "\n",
    "    def self_train(self, nruns=None):\n",
    "        self.t1 = time.time()\n",
    "        print(\"Training initial model\", end=' | ')\n",
    "        self.model = self.train_model(self.df_train, train_index=0)\n",
    "        \n",
    "        pca_save_path = self.working_dir.joinpath(f\"pca/0.jpg\")\n",
    "        dftrain_save_path = self.working_dir.joinpath(f\"df/0.jpg\")\n",
    "\n",
    "        do_pca(self.df_train, title=f\"0\", save_path=pca_save_path)\n",
    "\n",
    "        df_plot(self.df_train, title=f\"0\", save_path=dftrain_save_path)\n",
    "        \n",
    "        raw_runs = self.df_test.run.unique()\n",
    "        try:\n",
    "            oms = pd.read_csv(\"../data_raw/cmsoms_2017B.csv\")\n",
    "            \n",
    "            oms = oms[(oms['delivered_lumi']>1)&(oms['duration']>=3600)]\n",
    "            oms_runs = oms.run_number.unique()\n",
    "            exclude_runs = set(raw_runs) - set(oms_runs)\n",
    "            print(\"Excluding runs:\", exclude_runs)\n",
    "            raw_runs = list(set(raw_runs) - set(exclude_runs))\n",
    "        except:\n",
    "            print(\"Cannot compare with OMS runs\")\n",
    "            pass\n",
    "\n",
    "        runs = sorted(raw_runs)\n",
    "\n",
    "        if nruns:\n",
    "            runs = runs[:nruns]\n",
    "\n",
    "        for train_index, run_number in enumerate(runs):\n",
    "            print(f\"{train_index+1}. Working with run {run_number}\", end=' | ')\n",
    "\n",
    "            try:\n",
    "                # Dataset of a single run\n",
    "                df_run = self.df_test[self.df_test[\"run\"] == run_number].copy()\n",
    "\n",
    "                if len(df_run) == 0:\n",
    "                    print(f\"Run {run_number} has no data in test dataset\")\n",
    "                    continue\n",
    "\n",
    "                df_run = self.predict_run(df_run)\n",
    "\n",
    "                # Take a subset of only anomalies (y=2)\n",
    "                df_anomalies = df_run[df_run[\"y\"] == 2].copy()\n",
    "                if self.df_anomalies is None:\n",
    "                    self.df_anomalies = df_anomalies\n",
    "                else:\n",
    "                    self.df_anomalies = pd.concat([self.df_anomalies, df_anomalies],\n",
    "                                                  ignore_index=True, sort=False)\n",
    "\n",
    "                # Take a subset of only good and bad predictions, but no anomalies\n",
    "                df_confident = df_run[df_run[\"y\"] != 2].copy()\n",
    "\n",
    "                # Add new predictions to a training dataset\n",
    "                self.df_train = pd.concat([self.df_train, df_confident],\n",
    "                                          ignore_index=True, sort=False)\n",
    "\n",
    "                pca_df_save_path = self.working_dir.joinpath(f\"pca/{train_index+1}-{run_number}.jpg\")\n",
    "                pca_ta_save_path = self.working_dir.joinpath(f\"pca_ta/{train_index+1}-{run_number}.jpg\")\n",
    "                dftrain_save_path = self.working_dir.joinpath(f\"df/{train_index+1}-{run_number}.jpg\")\n",
    "\n",
    "                do_pca(self.df_train, bin_cols=self.bin_cols,\n",
    "                       title=f\"{train_index+1}\", save_path=pca_df_save_path)\n",
    "                \n",
    "                df_ta = pd.concat([self.df_train, self.df_anomalies],\n",
    "                          ignore_index=True, sort=False)\n",
    "                \n",
    "                df_plot2(df_ta, df_run, bin_cols=self.bin_cols, title1=f\"{train_index+1}\", title2=f\"{run_number}\", save_path=dftrain_save_path, show=False)\n",
    "                \n",
    "                do_pca(df_ta, bin_cols=self.bin_cols,\n",
    "                       title=f\"{train_index+1}\", save_path=pca_ta_save_path)\n",
    "                \n",
    "                self.model = self.train_model(self.df_train, train_index=train_index+1)\n",
    "            except Exception as err:\n",
    "                print(\"Failed self_train |\", err)\n",
    "\n",
    "        self.t2 = time.time()\n",
    "\n",
    "    def save(self):\n",
    "\n",
    "        self.model.save(self.working_dir.joinpath('model.h5'))\n",
    "\n",
    "        self.df_train.to_csv(self.working_dir.joinpath('df_train.csv'))\n",
    "        self.df_anomalies.to_csv(self.working_dir.joinpath('df_anomalies.csv'))\n",
    "\n",
    "        # Make a gif from pca images\n",
    "        # do_gif(self.working_dir)\n",
    "\n",
    "        df_ta = pd.concat([self.df_train, self.df_anomalies],\n",
    "                          ignore_index=True, sort=False)\n",
    "\n",
    "        # TSNE for classified dataset\n",
    "        do_tsne(self.df_train, bin_cols=self.bin_cols, save_path=self.working_dir.joinpath(\n",
    "            f\"tsne_df_train.jpg\"))\n",
    "\n",
    "        # TSNE for classified dataset + anomalies\n",
    "        do_tsne(df_ta, bin_cols=self.bin_cols, save_path=self.working_dir.joinpath(f\"tsne_df_ta.jpg\"))\n",
    "\n",
    "        # Plot histograms of classified dataset\n",
    "        df_plot(self.df_train, bin_cols=self.bin_cols, save_path=self.working_dir.joinpath(f\"df_train.jpg\"))\n",
    "\n",
    "        # Plot histograms of anomalies\n",
    "        df_plot(self.df_anomalies, bin_cols=self.bin_cols, save_path=self.working_dir.joinpath(\n",
    "            f\"df_anomalies.jpg\"))\n",
    "\n",
    "        # Plot histograms of classified dataset + anomalies\n",
    "        df_plot(df_ta, bin_cols=self.bin_cols, save_path=self.working_dir.joinpath(f\"df_ta.jpg\"))\n",
    "        \n",
    "        # Plot histograms of classified dataset + anomalies\n",
    "        do_pca(df_ta, bin_cols=self.bin_cols, save_path=self.working_dir.joinpath(f\"pca_df_ta.jpg\"))\n",
    "\n",
    "        self.t3 = time.time()\n",
    "        \n",
    "        # Time taken for each training\n",
    "        plt.figure(figsize=(20,20))\n",
    "        plt.plot([x[\"train_time\"] for x in self.training_performance], label=\"train_time\")\n",
    "        plt.legend()\n",
    "        plt.savefig(self.working_dir.joinpath(f\"duration.jpg\"))\n",
    "        plt.close()\n",
    "\n",
    "        self.training_performance = {\n",
    "            \"trainings\": self.training_performance,\n",
    "            \"duration\": {\n",
    "                \"train\": int(self.t2 - self.t1),\n",
    "                \"visual\": int(self.t3 - self.t2),\n",
    "                \"total\": int(self.t3 - self.t1)\n",
    "            }\n",
    "        }\n",
    "\n",
    "        with open(self.working_dir.joinpath('training_performance.json'), 'w') as fh:\n",
    "            json.dump(self.training_performance, fh)\n",
    "            \n",
    "        print(self.training_performance[\"duration\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working dir: ../trainings/data_small/t-0.9-20210103-180215\n",
      "Train set (834, 117) | Test set (26374, 116)\n",
      "Training initial model | Epoch 00134: early stopping\n",
      "Excluding runs: {297474, 297494, 297495, 297496, 297497, 297498, 297499, 297501, 297502, 297504, 299042, 299064, 299096, 297598, 297099, 298653, 299178, 299180, 299183, 299185, 298678, 297657, 297658, 297659, 297661, 297662, 297663, 297664, 297666, 297670, 297671, 297672, 297673, 297678, 297168, 297169, 297170, 297171, 297175, 297179, 297180, 297181, 297211, 297215, 297218, 297224, 297225, 299316, 299317, 299318, 299324, 299326, 299327, 297281, 297282, 297283, 297284, 297285, 297286, 297287, 297288, 297289, 297290, 297291, 297293, 297308, 297424, 297426, 297429, 297432, 297435, 298997, 298998, 299000, 297467, 297468, 297469}\n",
      "1. Working with run 297056 | Epoch 00101: early stopping\n",
      "2. Working with run 297057 | Epoch 00089: early stopping\n",
      "3. Working with run 297100 | Epoch 00091: early stopping\n",
      "4. Working with run 297101 | Epoch 00061: early stopping\n",
      "5. Working with run 297113 | Epoch 00057: early stopping\n",
      "6. Working with run 297114 | Epoch 00054: early stopping\n",
      "7. Working with run 297176 | Epoch 00085: early stopping\n",
      "8. Working with run 297177 | Epoch 00064: early stopping\n",
      "9. Working with run 297178 | Epoch 00089: early stopping\n",
      "10. Working with run 297219 | Epoch 00080: early stopping\n",
      "11. Working with run 297227 | Epoch 00078: early stopping\n",
      "12. Working with run 297292 | Epoch 00101: early stopping\n",
      "13. Working with run 297296 | Epoch 00084: early stopping\n",
      "14. Working with run 297359 | Epoch 00080: early stopping\n",
      "15. Working with run 297411 | Epoch 00087: early stopping\n",
      "16. Working with run 297425 | Epoch 00092: early stopping\n",
      "17. Working with run 297430 | Epoch 00099: early stopping\n",
      "18. Working with run 297431 | Epoch 00112: early stopping\n",
      "19. Working with run 297433 | Epoch 00077: early stopping\n",
      "20. Working with run 297434 | Epoch 00067: early stopping\n",
      "21. Working with run 297483 | Epoch 00114: early stopping\n",
      "22. Working with run 297484 | Epoch 00090: early stopping\n",
      "23. Working with run 297485 | Epoch 00083: early stopping\n",
      "24. Working with run 297486 | Epoch 00112: early stopping\n",
      "25. Working with run 297487 | Epoch 00113: early stopping\n",
      "26. Working with run 297488 | Epoch 00097: early stopping\n",
      "27. Working with run 297503 | Epoch 00121: early stopping\n",
      "28. Working with run 297505 | Epoch 00064: early stopping\n",
      "29. Working with run 297557 | Epoch 00131: early stopping\n",
      "30. Working with run 297558 | Epoch 00066: early stopping\n",
      "31. Working with run 297562 | Epoch 00099: early stopping\n",
      "32. Working with run 297563 | Epoch 00064: early stopping\n",
      "33. Working with run 297599 | Epoch 00112: early stopping\n",
      "34. Working with run 297603 | Epoch 00095: early stopping\n",
      "35. Working with run 297604 | Epoch 00059: early stopping\n",
      "36. Working with run 297605 | Epoch 00094: early stopping\n",
      "37. Working with run 297606 | Epoch 00096: early stopping\n",
      "38. Working with run 297620 | Epoch 00096: early stopping\n",
      "39. Working with run 297656 | Epoch 00087: early stopping\n",
      "40. Working with run 297660 | Epoch 00101: early stopping\n",
      "41. Working with run 297665 | Epoch 00093: early stopping\n",
      "42. Working with run 297674 | Epoch 00068: early stopping\n",
      "43. Working with run 297675 | Epoch 00075: early stopping\n",
      "44. Working with run 297722 | Epoch 00075: early stopping\n",
      "45. Working with run 297723 | Epoch 00082: early stopping\n",
      "46. Working with run 298996 | Epoch 00070: early stopping\n",
      "47. Working with run 299061 | Epoch 00086: early stopping\n",
      "48. Working with run 299062 | Epoch 00071: early stopping\n",
      "49. Working with run 299065 | Epoch 00087: early stopping\n",
      "50. Working with run 299067 | Epoch 00072: early stopping\n",
      "51. Working with run 299149 | Epoch 00072: early stopping\n",
      "52. Working with run 299184 | Epoch 00069: early stopping\n",
      "53. Working with run 299325 | Epoch 00062: early stopping\n",
      "54. Working with run 299329 | Epoch 00070: early stopping\n",
      "{'train': 1683, 'visual': 592, 'total': 2276}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    batch = False\n",
    "\n",
    "    # base_dir = Path('/eos/home-m/mantydze/ZeroBias2018B/scripts')\n",
    "    base_dir = Path('../')\n",
    "\n",
    "    if not batch:\n",
    "        data_dir = \"data_small\"\n",
    "        box = BlackBox(data_dir=base_dir.joinpath(data_dir),\n",
    "                       working_dir=base_dir.joinpath(\"trainings\").joinpath(data_dir),\n",
    "                       threshold=0.9)\n",
    "        box.load_df(nbins=60, step=2)\n",
    "        box.self_train()\n",
    "#         box.self_train(nruns=25)\n",
    "        box.save()\n",
    "\n",
    "    #         exit()\n",
    "\n",
    "    else:\n",
    "        data_dirs = [\"data_0\", \"data_ext\", \"data_small\"]\n",
    "        thresholds = sorted([0.5, 0.6, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95], reverse=True)\n",
    "\n",
    "#         data_dirs = [\"data_small\"]\n",
    "        thresholds = [0.7, 0.8, 0.9]\n",
    "#         thresholds = [0.5, 0.6, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n",
    "\n",
    "        for data_dir in data_dirs:\n",
    "            for threshold in thresholds:\n",
    "                box = BlackBox(data_dir=base_dir.joinpath(data_dir),\n",
    "                            working_dir=base_dir.joinpath(\"trainings\").joinpath(data_dir),\n",
    "                            threshold=threshold)\n",
    "                box.load_df(nbins=60, step=2)\n",
    "    #                 box.self_train(nruns=25)\n",
    "                box.self_train()\n",
    "                box.save()\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir ../trainings_test/"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "499px",
    "left": "1848px",
    "right": "20px",
    "top": "113px",
    "width": "694px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
